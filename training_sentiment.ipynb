{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ab0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14265b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: C:\\Users\\muham\\Project\\nlp-ki\n",
      "Dataset Path: C:\\Users\\muham\\Project\\nlp-ki\\dataset\\googleplaystore_user_reviews.csv\n",
      "Model Save Path: C:\\Users\\muham\\Project\\nlp-ki\\saved_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Set base path - use notebook directory\n",
    "BASE_PATH = os.path.dirname(os.path.abspath('__file__')) if '__file__' in dir() else os.getcwd()\n",
    "DATASET_PATH = os.path.join(BASE_PATH, 'dataset', 'googleplaystore_user_reviews.csv')\n",
    "MODEL_SAVE_PATH = os.path.join(BASE_PATH, 'saved_model')\n",
    "\n",
    "print(f\"Base Path: {BASE_PATH}\")\n",
    "print(f\"Dataset Path: {DATASET_PATH}\")\n",
    "print(f\"Model Save Path: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36ff9a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (64295, 5)\n",
      "\n",
      "Columns: ['App', 'Translated_Review', 'Sentiment', 'Sentiment_Polarity', 'Sentiment_Subjectivity']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>App</th>\n",
       "      <th>Translated_Review</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Polarity</th>\n",
       "      <th>Sentiment_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>I like eat delicious food. That's I'm cooking ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>This help eating healthy exercise regular basis</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.288462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Works great especially going grocery store</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Best idea us</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     App                                  Translated_Review  \\\n",
       "0  10 Best Foods for You  I like eat delicious food. That's I'm cooking ...   \n",
       "1  10 Best Foods for You    This help eating healthy exercise regular basis   \n",
       "2  10 Best Foods for You                                                NaN   \n",
       "3  10 Best Foods for You         Works great especially going grocery store   \n",
       "4  10 Best Foods for You                                       Best idea us   \n",
       "\n",
       "  Sentiment  Sentiment_Polarity  Sentiment_Subjectivity  \n",
       "0  Positive                1.00                0.533333  \n",
       "1  Positive                0.25                0.288462  \n",
       "2       NaN                 NaN                     NaN  \n",
       "3  Positive                0.40                0.875000  \n",
       "4  Positive                1.00                0.300000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f18952f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking data quality...\n",
      "Null values per column:\n",
      "App                           0\n",
      "Translated_Review         26868\n",
      "Sentiment                 26863\n",
      "Sentiment_Polarity        26863\n",
      "Sentiment_Subjectivity    26863\n",
      "dtype: int64\n",
      "\n",
      "Sentiment distribution:\n",
      "Sentiment\n",
      "Positive    23998\n",
      "Negative     8271\n",
      "Neutral      5163\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for required columns\n",
    "print(f\"\\nChecking data quality...\")\n",
    "print(f\"Null values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fbb0994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned dataset shape: (33222, 7)\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "2    21240\n",
      "0     7963\n",
      "1     4019\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "# Drop rows with missing reviews or sentiment\n",
    "df = df.dropna(subset=['Translated_Review', 'Sentiment'])\n",
    "\n",
    "# Convert sentiment to numeric labels\n",
    "sentiment_map = {\n",
    "    'Positive': 2,\n",
    "    'Neutral': 1,\n",
    "    'Negative': 0\n",
    "}\n",
    "\n",
    "df['label'] = df['Sentiment'].map(sentiment_map)\n",
    "df = df.dropna(subset=['label'])\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Rename text column for clarity\n",
    "df['text'] = df['Translated_Review']\n",
    "\n",
    "# Filter out very short reviews (less than 3 words)\n",
    "df = df[df['text'].str.split().str.len() >= 3]\n",
    "\n",
    "print(f\"\\nCleaned dataset shape: {df.shape}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ea4a661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balancing dataset to 4019 samples per class...\n",
      "Balanced dataset shape: (12057, 7)\n",
      "\n",
      "Balanced label distribution:\n",
      "label\n",
      "2    4019\n",
      "0    4019\n",
      "1    4019\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Balance dataset (optional - sample equal amounts from each class)\n",
    "min_count = df['label'].value_counts().min()\n",
    "print(f\"\\nBalancing dataset to {min_count} samples per class...\")\n",
    "\n",
    "df_balanced = df.groupby('label').sample(n=min(min_count, 10000), random_state=42)\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Balanced dataset shape: {df_balanced.shape}\")\n",
    "print(f\"\\nBalanced label distribution:\")\n",
    "print(df_balanced['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08d4badb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 9645\n",
      "Validation samples: 2412\n"
     ]
    }
   ],
   "source": [
    "# Train-validation split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_balanced['text'].tolist(),\n",
    "    df_balanced['label'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_balanced['label']\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Validation samples: {len(val_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fc72fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer: roberta-base\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "model_name = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Loaded tokenizer: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03582d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff03c4aa7f8946269286486358a9569a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9645 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813b33679a5f481390df1ebf03e31e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets tokenized and formatted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels})\n",
    "val_dataset = Dataset.from_dict({'text': val_texts, 'label': val_labels})\n",
    "\n",
    "# Tokenize\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "print(\"Datasets tokenized and formatted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77d47eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RoBERTa model (this may take a few minutes for first download)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Using device: cpu\n",
      "No GPU detected. Training will use CPU (this will be slower).\n"
     ]
    }
   ],
   "source": [
    "# Load model for sequence classification\n",
    "print(\"Loading RoBERTa model (this may take a few minutes for first download)...\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3\n",
    ")\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU detected. Training will use CPU (this will be slower).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "167543c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted'\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64393d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration set!\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(BASE_PATH, 'training_output'),\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=os.path.join(BASE_PATH, 'logs'),\n",
    "    logging_steps=100,\n",
    "    eval_strategy='epoch',  # Changed from evaluation_strategy\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "print(\"Training configuration set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebef2d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muham\\AppData\\Local\\Temp\\ipykernel_11744\\3179816529.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f93baf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Starting model training...\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muham\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1809' max='1809' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1809/1809 1:10:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.464500</td>\n",
       "      <td>0.416800</td>\n",
       "      <td>0.856965</td>\n",
       "      <td>0.857479</td>\n",
       "      <td>0.865075</td>\n",
       "      <td>0.856965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.302600</td>\n",
       "      <td>0.318124</td>\n",
       "      <td>0.906302</td>\n",
       "      <td>0.906511</td>\n",
       "      <td>0.907487</td>\n",
       "      <td>0.906302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.172300</td>\n",
       "      <td>0.321639</td>\n",
       "      <td>0.925788</td>\n",
       "      <td>0.926004</td>\n",
       "      <td>0.927106</td>\n",
       "      <td>0.925788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muham\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\muham\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\muham\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1809, training_loss=0.40225221505805625, metrics={'train_runtime': 4210.7898, 'train_samples_per_second': 6.872, 'train_steps_per_second': 0.43, 'total_flos': 1903296685489920.0, 'train_loss': 0.40225221505805625, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting model training...\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0c85519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Evaluating model on validation set...\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muham\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 01:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "eval_loss: 0.3216\n",
      "eval_accuracy: 0.9258\n",
      "eval_f1: 0.9260\n",
      "eval_precision: 0.9271\n",
      "eval_recall: 0.9258\n",
      "eval_runtime: 70.6405\n",
      "eval_samples_per_second: 34.1450\n",
      "eval_steps_per_second: 1.0760\n",
      "epoch: 3.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Evaluating model on validation set...\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "857c47ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to: C:\\Users\\muham\\Project\\nlp-ki\\saved_model\n",
      "\n",
      "==================================================\n",
      "Model and tokenizer saved successfully!\n",
      "==================================================\n",
      "\n",
      "Saved files: ['config.json', 'merges.txt', 'model.safetensors', 'special_tokens_map.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.json']\n",
      "\n",
      "==================================================\n",
      "Model and tokenizer saved successfully!\n",
      "==================================================\n",
      "\n",
      "Saved files: ['config.json', 'merges.txt', 'model.safetensors', 'special_tokens_map.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.json']\n"
     ]
    }
   ],
   "source": [
    "# Create directory if it doesn't exist\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Save model and tokenizer\n",
    "print(f\"\\nSaving model to: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "trainer.save_model(MODEL_SAVE_PATH)\n",
    "tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Model and tokenizer saved successfully!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Verify saved files\n",
    "saved_files = os.listdir(MODEL_SAVE_PATH)\n",
    "print(f\"\\nSaved files: {saved_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "431e37e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Predictions:\n",
      "==================================================\n",
      "Text: This app is amazing! I love it so much!\n",
      "Predicted Sentiment: Positive\n",
      "--------------------------------------------------\n",
      "Text: The app keeps crashing. Very disappointed.\n",
      "Predicted Sentiment: Negative\n",
      "--------------------------------------------------\n",
      "Text: It's okay, nothing special but works fine.\n",
      "Predicted Sentiment: Positive\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test with sample predictions\n",
    "test_texts = [\n",
    "    \"This app is amazing! I love it so much!\",\n",
    "    \"The app keeps crashing. Very disappointed.\",\n",
    "    \"It's okay, nothing special but works fine.\"\n",
    "]\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(test_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Move to device\n",
    "if device.type == 'cuda':\n",
    "    model = model.to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Map predictions back to labels\n",
    "label_map = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "\n",
    "print(\"\\nTest Predictions:\")\n",
    "print(\"=\"*50)\n",
    "for text, pred in zip(test_texts, predictions):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Sentiment: {label_map[pred.item()]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf9dc632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing app search...\n",
      "Found 3 apps\n",
      "- Instagram (None)\n",
      "- Threads (com.instagram.barcelona)\n",
      "- Edits: Video Editor (com.instagram.basel)\n",
      "\n",
      "Testing review scraping...\n",
      "Found 3 apps\n",
      "- Instagram (None)\n",
      "- Threads (com.instagram.barcelona)\n",
      "- Edits: Video Editor (com.instagram.basel)\n",
      "\n",
      "Testing review scraping...\n",
      "Fetched 10 reviews\n",
      "First review: exhalent for Instagram thanks...\n",
      "Fetched 10 reviews\n",
      "First review: exhalent for Instagram thanks...\n"
     ]
    }
   ],
   "source": [
    "# Test Google Play Scraper\n",
    "from google_play_scraper import app, search, Sort, reviews\n",
    "\n",
    "# Test search\n",
    "print(\"Testing app search...\")\n",
    "try:\n",
    "    results = search(\"instagram\", lang='en', country='us', n_hits=3)\n",
    "    print(f\"Found {len(results)} apps\")\n",
    "    for r in results[:3]:\n",
    "        print(f\"- {r['title']} ({r['appId']})\")\n",
    "except Exception as e:\n",
    "    print(f\"Search error: {e}\")\n",
    "\n",
    "# Test review scraping\n",
    "print(\"\\nTesting review scraping...\")\n",
    "try:\n",
    "    app_id = \"com.instagram.android\"  # Instagram app ID\n",
    "    result, token = reviews(\n",
    "        app_id,\n",
    "        lang='en',\n",
    "        country='us',\n",
    "        sort=Sort.NEWEST,\n",
    "        count=10\n",
    "    )\n",
    "    print(f\"Fetched {len(result)} reviews\")\n",
    "    if result:\n",
    "        print(f\"First review: {result[0]['content'][:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Scraping error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7774dd3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
