{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ed66668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: C:\\Users\\muham\\Project\\nlp-ki\n",
      "Model Save Path: C:\\Users\\muham\\Project\\nlp-ki\\saved_model_id\n",
      "PyTorch Version: 2.7.1+cu118\n",
      "CUDA Available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "\n",
    "# Setup paths\n",
    "BASE_PATH = r'C:\\Users\\muham\\Project\\nlp-ki'\n",
    "MODEL_SAVE_PATH = os.path.join(BASE_PATH, 'saved_model_id')\n",
    "\n",
    "print(f\"Base Path: {BASE_PATH}\")\n",
    "print(f\"Model Save Path: {MODEL_SAVE_PATH}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd43a7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Indonesian SMSA sentiment dataset from GitHub...\n",
      "\n",
      "‚úÖ Dataset loaded successfully!\n",
      "Train samples: 11000\n",
      "Validation samples: 1260\n",
      "Test samples: 500\n",
      "\n",
      "Columns: ['text', 'label']\n",
      "\n",
      "First few rows:\n",
      "                                                text     label\n",
      "0  warung ini dimiliki oleh pengusaha pabrik tahu...  positive\n",
      "1  mohon ulama lurus dan k212 mmbri hujjah partai...   neutral\n",
      "2  lokasi strategis di jalan sumatera bandung . t...  positive\n",
      "3  betapa bahagia nya diri ini saat unboxing pake...  positive\n",
      "4  duh . jadi mahasiswa jangan sombong dong . kas...  negative\n",
      "\n",
      "Label distribution in training set:\n",
      "label\n",
      "positive    6416\n",
      "negative    3436\n",
      "neutral     1148\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚ö†Ô∏è Converting string labels to numeric (positive=2, neutral=1, negative=0)\n",
      "\n",
      "Numeric label distribution:\n",
      "label\n",
      "0    3436\n",
      "1    1148\n",
      "2    6416\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÖ Dataset converted to HuggingFace format!\n",
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 11000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n",
      "\n",
      "‚úÖ Dataset loaded successfully!\n",
      "Train samples: 11000\n",
      "Validation samples: 1260\n",
      "Test samples: 500\n",
      "\n",
      "Columns: ['text', 'label']\n",
      "\n",
      "First few rows:\n",
      "                                                text     label\n",
      "0  warung ini dimiliki oleh pengusaha pabrik tahu...  positive\n",
      "1  mohon ulama lurus dan k212 mmbri hujjah partai...   neutral\n",
      "2  lokasi strategis di jalan sumatera bandung . t...  positive\n",
      "3  betapa bahagia nya diri ini saat unboxing pake...  positive\n",
      "4  duh . jadi mahasiswa jangan sombong dong . kas...  negative\n",
      "\n",
      "Label distribution in training set:\n",
      "label\n",
      "positive    6416\n",
      "negative    3436\n",
      "neutral     1148\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚ö†Ô∏è Converting string labels to numeric (positive=2, neutral=1, negative=0)\n",
      "\n",
      "Numeric label distribution:\n",
      "label\n",
      "0    3436\n",
      "1    1148\n",
      "2    6416\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÖ Dataset converted to HuggingFace format!\n",
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 11000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load Indonesian sentiment dataset langsung dari GitHub (bypass HF dataset script)\n",
    "print(\"Loading Indonesian SMSA sentiment dataset from GitHub...\")\n",
    "\n",
    "# URL dataset dari IndoNLU GitHub repository\n",
    "train_url = \"https://raw.githubusercontent.com/IndoNLP/indonlu/master/dataset/smsa_doc-sentiment-prosa/train_preprocess.tsv\"\n",
    "valid_url = \"https://raw.githubusercontent.com/IndoNLP/indonlu/master/dataset/smsa_doc-sentiment-prosa/valid_preprocess.tsv\"\n",
    "test_url = \"https://raw.githubusercontent.com/IndoNLP/indonlu/master/dataset/smsa_doc-sentiment-prosa/test_preprocess.tsv\"\n",
    "\n",
    "# Load dari TSV files (no header in file)\n",
    "train_df = pd.read_csv(train_url, sep='\\t', header=None, names=['text', 'label'])\n",
    "valid_df = pd.read_csv(valid_url, sep='\\t', header=None, names=['text', 'label'])\n",
    "test_df = pd.read_csv(test_url, sep='\\t', header=None, names=['text', 'label'])\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(valid_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# Lihat struktur data\n",
    "print(f\"\\nColumns: {train_df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(train_df.head())\n",
    "\n",
    "# Cek label distribution\n",
    "print(f\"\\nLabel distribution in training set:\")\n",
    "print(train_df['label'].value_counts())\n",
    "\n",
    "# Map string labels ke numeric\n",
    "label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "print(\"\\n‚ö†Ô∏è Converting string labels to numeric (positive=2, neutral=1, negative=0)\")\n",
    "train_df['label'] = train_df['label'].map(label_map)\n",
    "valid_df['label'] = valid_df['label'].map(label_map)\n",
    "test_df['label'] = test_df['label'].map(label_map)\n",
    "print(f\"\\nNumeric label distribution:\")\n",
    "print(train_df['label'].value_counts().sort_index())\n",
    "\n",
    "# Konversi ke HuggingFace Dataset format\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'validation': Dataset.from_pandas(valid_df),\n",
    "    'test': Dataset.from_pandas(test_df)\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset converted to HuggingFace format!\")\n",
    "print(f\"Dataset structure: {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fdcb641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slang dictionary loaded: 88 entries\n",
      "\n",
      "Contoh slang normalization:\n",
      "  gak -> tidak\n",
      "  ga -> tidak\n",
      "  gk -> tidak\n",
      "  ngga -> tidak\n",
      "  nggak -> tidak\n",
      "  tdk -> tidak\n",
      "  yg -> yang\n",
      "  dgn -> dengan\n",
      "  utk -> untuk\n",
      "  dg -> dengan\n"
     ]
    }
   ],
   "source": [
    "# Dictionary untuk normalisasi kata slang/informal Bahasa Indonesia\n",
    "slang_dict = {\n",
    "    'gak': 'tidak',\n",
    "    'ga': 'tidak',\n",
    "    'gk': 'tidak',\n",
    "    'ngga': 'tidak',\n",
    "    'nggak': 'tidak',\n",
    "    'tdk': 'tidak',\n",
    "    'yg': 'yang',\n",
    "    'dgn': 'dengan',\n",
    "    'utk': 'untuk',\n",
    "    'dg': 'dengan',\n",
    "    'krn': 'karena',\n",
    "    'bgt': 'banget',\n",
    "    'bgt': 'banget',\n",
    "    'bgt': 'banget',\n",
    "    'bgd': 'banget',\n",
    "    'bngt': 'banget',\n",
    "    'bener': 'benar',\n",
    "    'bner': 'benar',\n",
    "    'kalo': 'kalau',\n",
    "    'klo': 'kalau',\n",
    "    'udah': 'sudah',\n",
    "    'udh': 'sudah',\n",
    "    'dah': 'sudah',\n",
    "    'emang': 'memang',\n",
    "    'emg': 'memang',\n",
    "    'hrs': 'harus',\n",
    "    'gue': 'saya',\n",
    "    'gw': 'saya',\n",
    "    'ane': 'saya',\n",
    "    'aku': 'saya',\n",
    "    'lu': 'kamu',\n",
    "    'lo': 'kamu',\n",
    "    'ente': 'kamu',\n",
    "    'jd': 'jadi',\n",
    "    'jdi': 'jadi',\n",
    "    'kyk': 'seperti',\n",
    "    'kayak': 'seperti',\n",
    "    'gmn': 'bagaimana',\n",
    "    'gimana': 'bagaimana',\n",
    "    'gmana': 'bagaimana',\n",
    "    'knp': 'kenapa',\n",
    "    'knapa': 'kenapa',\n",
    "    'td': 'tadi',\n",
    "    'skrg': 'sekarang',\n",
    "    'skrng': 'sekarang',\n",
    "    'ajah': 'saja',\n",
    "    'aja': 'saja',\n",
    "    'aj': 'saja',\n",
    "    'gitu': 'begitu',\n",
    "    'gt': 'begitu',\n",
    "    'bkin': 'bikin',\n",
    "    'bikin': 'membuat',\n",
    "    'sampe': 'sampai',\n",
    "    'smp': 'sampai',\n",
    "    'tp': 'tetapi',\n",
    "    'tapi': 'tetapi',\n",
    "    'org': 'orang',\n",
    "    'orng': 'orang',\n",
    "    'blm': 'belum',\n",
    "    'blom': 'belum',\n",
    "    'kpn': 'kapan',\n",
    "    'kapan': 'kapan',\n",
    "    'pgn': 'ingin',\n",
    "    'pgen': 'ingin',\n",
    "    'pengen': 'ingin',\n",
    "    'mo': 'mau',\n",
    "    'mau': 'ingin',\n",
    "    'ada': 'ada',\n",
    "    'adanya': 'ada',\n",
    "    'nyebelin': 'menyebalkan',\n",
    "    'sebel': 'kesal',\n",
    "    'bete': 'kesal',\n",
    "    'males': 'malas',\n",
    "    'cape': 'capek',\n",
    "    'capek': 'lelah',\n",
    "    'mantap': 'bagus',\n",
    "    'mantep': 'bagus',\n",
    "    'keren': 'bagus',\n",
    "    'jelek': 'buruk',\n",
    "    'ancur': 'hancur',\n",
    "    'parah': 'buruk',\n",
    "    'lemot': 'lambat',\n",
    "    'lelet': 'lambat',\n",
    "    'eror': 'error',\n",
    "    'error': 'kesalahan',\n",
    "    'crash': 'rusak',\n",
    "    'ngecrash': 'rusak',\n",
    "    'ngelag': 'lambat',\n",
    "    'lag': 'lambat',\n",
    "    'lemot': 'lambat',\n",
    "    'loadingnya': 'loading'\n",
    "}\n",
    "\n",
    "print(f\"Slang dictionary loaded: {len(slang_dict)} entries\")\n",
    "print(\"\\nContoh slang normalization:\")\n",
    "for key, value in list(slang_dict.items())[:10]:\n",
    "    print(f\"  {key} -> {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f7df79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing preprocessing function:\n",
      "======================================================================\n",
      "Original: Aplikasi ini bgtttt keren!!! üëçüëçüëç\n",
      "Cleaned:  aplikasi ini bgtttt keren!\n",
      "----------------------------------------------------------------------\n",
      "Original: Gak bisa dibuka, crash terus @developer #disappointed üò°\n",
      "Cleaned:  tidak bisa dibuka, rusak terus\n",
      "----------------------------------------------------------------------\n",
      "Original: Biasa aja sih, gak ada yg spesial www.example.com\n",
      "Cleaned:  biasa saja sih, tidak ada yang spesial\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocessing text untuk Bahasa Indonesia\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw text input\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned and preprocessed text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Case folding - convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 3. Remove mentions (@username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # 4. Remove hashtags (#hashtag)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    \n",
    "    # 5. Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 6. Remove excessive punctuation (keep single punctuation)\n",
    "    text = re.sub(r'([!?.,])\\1+', r'\\1', text)\n",
    "    \n",
    "    # 7. Remove special characters (keep letters and basic punctuation)\n",
    "    text = re.sub(r'[^a-zA-Z\\s!?.,]', '', text)\n",
    "    \n",
    "    # 8. Normalize slang words\n",
    "    words = text.split()\n",
    "    normalized_words = [slang_dict.get(word, word) for word in words]\n",
    "    text = ' '.join(normalized_words)\n",
    "    \n",
    "    # 9. Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test preprocessing function\n",
    "test_texts = [\n",
    "    \"Aplikasi ini bgtttt keren!!! üëçüëçüëç\",\n",
    "    \"Gak bisa dibuka, crash terus @developer #disappointed üò°\",\n",
    "    \"Biasa aja sih, gak ada yg spesial www.example.com\"\n",
    "]\n",
    "\n",
    "print(\"Testing preprocessing function:\")\n",
    "print(\"=\"*70)\n",
    "for text in test_texts:\n",
    "    cleaned = preprocess_text(text)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Cleaned:  {cleaned}\")\n",
    "    print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e96abd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying preprocessing to all splits...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc688829a7d451a980f1c9011af4f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43d5c27d6754ed892274d86653fc2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a707457e8ed46ebba0b5c3e5c76b23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing complete!\n",
      "\n",
      "Contoh hasil preprocessing:\n",
      "\n",
      "Sample 1:\n",
      "Text: warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung . tahu berkualitas , dipadu keahlian memasak , dipadu kretivitas , jadilah warung yang menyajikan menu utama berbahan tahu , ditambah menu umum lain seperti ayam . semuanya selera indonesia . harga cukup terjangkau . jangan lewatkan tahu bletoka nya , tidak kalah dengan yang asli dari tegal !\n",
      "Label: 2\n",
      "\n",
      "Sample 2:\n",
      "Text: mohon ulama lurus dan k mmbri hujjah partai apa yang harus diwlh agar suara islam tidak pecahpecah\n",
      "Label: 1\n",
      "\n",
      "Sample 3:\n",
      "Text: lokasi strategis di jalan sumatera bandung . tempat nya nyaman terutama sofa di lantai . paella nya enak , sangat pas dimakan dengan minum bir dingin . appetiser nya juga enakenak .\n",
      "Label: 2\n"
     ]
    }
   ],
   "source": [
    "def preprocess_dataset(examples):\n",
    "    \"\"\"\n",
    "    Apply preprocessing to dataset examples\n",
    "    \"\"\"\n",
    "    examples['text'] = [preprocess_text(text) for text in examples['text']]\n",
    "    return examples\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Applying preprocessing to all splits...\")\n",
    "\n",
    "dataset = dataset.map(preprocess_dataset, batched=True)\n",
    "\n",
    "print(\"\\nPreprocessing complete!\")\n",
    "print(\"\\nContoh hasil preprocessing:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Text: {dataset['train'][i]['text']}\")\n",
    "    print(f\"Label: {dataset['train'][i]['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7430b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: indobenchmark/indobert-base-p1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cea4ade0f49492196ee7f955434d4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muham\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\muham\\.cache\\huggingface\\hub\\models--indobenchmark--indobert-base-p1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5bcb2b2c1c64931b6e8ef012b891f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67e44bf4c8f49cca0cad270f2a9d0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30a42cd4723486197bdd0a7a988743a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: indobenchmark/indobert-base-p1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbe7712a65c4ad69f1702ce67cd957c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Model name\n",
    "model_name = 'indobenchmark/indobert-base-p1'\n",
    "\n",
    "print(f\"Loading tokenizer: {model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3  # positive, neutral, negative\n",
    ")\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc35aa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919010d0b78a42a3b28ff39c09645838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0487654a3f435d9681caccad0c87ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51a620ac5cb451abc51b0177fff3e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization complete!\n",
      "Train dataset: 11000 samples\n",
      "Validation dataset: 1260 samples\n",
      "Test dataset: 500 samples\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize text examples\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128  # Maksimal panjang sequence\n",
    "    )\n",
    "\n",
    "# Tokenize all splits\n",
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_dataset.set_format(\n",
    "    'torch', \n",
    "    columns=['input_ids', 'attention_mask', 'label']\n",
    ")\n",
    "\n",
    "print(\"\\nTokenization complete!\")\n",
    "print(f\"Train dataset: {len(tokenized_dataset['train'])} samples\")\n",
    "print(f\"Validation dataset: {len(tokenized_dataset['validation'])} samples\")\n",
    "print(f\"Test dataset: {len(tokenized_dataset['test'])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9757d903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics function defined!\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute accuracy and F1-score for evaluation\n",
    "    \n",
    "    Args:\n",
    "        eval_pred: Tuple of (predictions, labels)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing accuracy and f1 scores\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "    f1_macro = f1_score(labels, predictions, average='macro')\n",
    "    \n",
    "    # Detailed metrics per class\n",
    "    precision, recall, f1_per_class, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_negative': f1_per_class[0],\n",
    "        'f1_neutral': f1_per_class[1],\n",
    "        'f1_positive': f1_per_class[2],\n",
    "    }\n",
    "\n",
    "print(\"Metrics function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43a26465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  - Epochs: 3\n",
      "  - Batch size: 16\n",
      "  - Learning rate: 2e-05\n",
      "  - Weight decay: 0.01\n",
      "  - Device: CPU\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "output_dir = os.path.join(BASE_PATH, 'training_output_id')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    logging_dir=os.path.join(BASE_PATH, 'logs_id'),\n",
    "    logging_steps=100,\n",
    "    eval_strategy='epoch',  # Changed from evaluation_strategy\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_weighted',\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    use_cpu=not torch.cuda.is_available(),  # Changed from fp16\n",
    "    report_to='none',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  - Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  - Weight decay: {training_args.weight_decay}\")\n",
    "print(f\"  - Device: {'CPU' if training_args.use_cpu else 'GPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "992657f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized successfully!\n",
      "\n",
      "Total training steps: 2061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muham\\AppData\\Local\\Temp\\ipykernel_35192\\596881500.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "print(f\"\\nTotal training steps: {len(tokenized_dataset['train']) // training_args.per_device_train_batch_size * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a06ee679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ STARTING MODEL TRAINING...\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2064' max='2064' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2064/2064 1:19:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Negative</th>\n",
       "      <th>F1 Neutral</th>\n",
       "      <th>F1 Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.223900</td>\n",
       "      <td>0.234812</td>\n",
       "      <td>0.927778</td>\n",
       "      <td>0.926387</td>\n",
       "      <td>0.893464</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.815451</td>\n",
       "      <td>0.954943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.162900</td>\n",
       "      <td>0.192293</td>\n",
       "      <td>0.940476</td>\n",
       "      <td>0.940232</td>\n",
       "      <td>0.915575</td>\n",
       "      <td>0.929936</td>\n",
       "      <td>0.856031</td>\n",
       "      <td>0.960758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.250183</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.938511</td>\n",
       "      <td>0.914150</td>\n",
       "      <td>0.929193</td>\n",
       "      <td>0.854839</td>\n",
       "      <td>0.958419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÖ TRAINING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Training metrics:\n",
      "  train_runtime: 4760.0105\n",
      "  train_samples_per_second: 6.9330\n",
      "  train_steps_per_second: 0.4340\n",
      "  total_flos: 2170685696256000.0000\n",
      "  train_loss: 0.1790\n",
      "  epoch: 3.0000\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ STARTING MODEL TRAINING...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Train the model\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Print training summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nTraining metrics:\")\n",
    "for key, value in train_result.metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cfa237e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä EVALUATING ON VALIDATION SET...\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 01:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "  eval_loss: 0.1923\n",
      "  eval_accuracy: 0.9405\n",
      "  eval_f1_weighted: 0.9402\n",
      "  eval_f1_macro: 0.9156\n",
      "  eval_f1_negative: 0.9299\n",
      "  eval_f1_neutral: 0.8560\n",
      "  eval_f1_positive: 0.9608\n",
      "  eval_runtime: 35.4697\n",
      "  eval_samples_per_second: 35.5230\n",
      "  eval_steps_per_second: 1.1280\n",
      "  epoch: 3.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä EVALUATING ON VALIDATION SET...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1c3e67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîç EVALUATING ON TEST SET...\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Test Results:\n",
      "  eval_loss: 0.2921\n",
      "  eval_accuracy: 0.9080\n",
      "  eval_f1_weighted: 0.9050\n",
      "  eval_f1_macro: 0.8825\n",
      "  eval_f1_negative: 0.9381\n",
      "  eval_f1_neutral: 0.7871\n",
      "  eval_f1_positive: 0.9224\n",
      "  eval_runtime: 13.7832\n",
      "  eval_samples_per_second: 36.2760\n",
      "  eval_steps_per_second: 1.1610\n",
      "  epoch: 3.0000\n",
      "\n",
      "Test Results:\n",
      "  eval_loss: 0.2921\n",
      "  eval_accuracy: 0.9080\n",
      "  eval_f1_weighted: 0.9050\n",
      "  eval_f1_macro: 0.8825\n",
      "  eval_f1_negative: 0.9381\n",
      "  eval_f1_neutral: 0.7871\n",
      "  eval_f1_positive: 0.9224\n",
      "  eval_runtime: 13.7832\n",
      "  eval_samples_per_second: 36.2760\n",
      "  eval_steps_per_second: 1.1610\n",
      "  epoch: 3.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîç EVALUATING ON TEST SET...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "test_results = trainer.evaluate(tokenized_dataset['test'])\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "for key, value in test_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8e8a78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üíæ SAVING MODEL AND TOKENIZER...\n",
      "======================================================================\n",
      "\n",
      "Saving model to: C:\\Users\\muham\\Project\\nlp-ki\\saved_model_id\n",
      "\n",
      "======================================================================\n",
      "‚úÖ MODEL AND TOKENIZER SAVED SUCCESSFULLY!\n",
      "======================================================================\n",
      "\n",
      "Saved files (7 files):\n",
      "  - config.json\n",
      "  - model.safetensors\n",
      "  - special_tokens_map.json\n",
      "  - tokenizer.json\n",
      "  - tokenizer_config.json\n",
      "  - training_args.bin\n",
      "  - vocab.txt\n",
      "\n",
      "======================================================================\n",
      "‚úÖ MODEL AND TOKENIZER SAVED SUCCESSFULLY!\n",
      "======================================================================\n",
      "\n",
      "Saved files (7 files):\n",
      "  - config.json\n",
      "  - model.safetensors\n",
      "  - special_tokens_map.json\n",
      "  - tokenizer.json\n",
      "  - tokenizer_config.json\n",
      "  - training_args.bin\n",
      "  - vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# CRITICAL: Save model to specific path\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üíæ SAVING MODEL AND TOKENIZER...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "print(f\"\\nSaving model to: {MODEL_SAVE_PATH}\")\n",
    "trainer.save_model(MODEL_SAVE_PATH)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ MODEL AND TOKENIZER SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verify saved files\n",
    "saved_files = os.listdir(MODEL_SAVE_PATH)\n",
    "print(f\"\\nSaved files ({len(saved_files)} files):\")\n",
    "for file in saved_files[:10]:  # Show first 10 files\n",
    "    print(f\"  - {file}\")\n",
    "if len(saved_files) > 10:\n",
    "    print(f\"  ... and {len(saved_files) - 10} more files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59ce7734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîÑ LOADING MODEL FROM SAVED PATH FOR TESTING...\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Model loaded successfully from: C:\\Users\\muham\\Project\\nlp-ki\\saved_model_id\n"
     ]
    }
   ],
   "source": [
    "# Load model from saved path\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîÑ LOADING MODEL FROM SAVED PATH FOR TESTING...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load tokenizer and model\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(MODEL_SAVE_PATH)\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "# Move to device\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded successfully from: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c4422ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üß™ TESTING INFERENCE WITH MANUAL SENTENCES\n",
      "======================================================================\n",
      "\n",
      "Test 1:\n",
      "  Original: Aplikasi ini sangat membantu, fiturnya lengkap.\n",
      "  Cleaned:  aplikasi ini sangat membantu, fiturnya lengkap.\n",
      "  Predicted Sentiment: Positive\n",
      "  Confidence Scores:\n",
      "    - Negative: 0.0006 (0.06%)\n",
      "    - Neutral:  0.0013 (0.13%)\n",
      "    - Positive: 0.9980 (99.80%)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Test 2:\n",
      "  Original: Sering crash pas dibuka, nyesel download.\n",
      "  Cleaned:  sering rusak pas dibuka, nyesel download.\n",
      "  Predicted Sentiment: Negative\n",
      "  Confidence Scores:\n",
      "    - Negative: 0.9956 (99.56%)\n",
      "    - Neutral:  0.0009 (0.09%)\n",
      "    - Positive: 0.0034 (0.34%)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Test 3:\n",
      "  Original: Biasa aja sih, standar.\n",
      "  Cleaned:  biasa saja sih, standar.\n",
      "  Predicted Sentiment: Negative\n",
      "  Confidence Scores:\n",
      "    - Negative: 0.9931 (99.31%)\n",
      "    - Neutral:  0.0024 (0.24%)\n",
      "    - Positive: 0.0045 (0.45%)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "‚úÖ INFERENCE TESTING COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"Aplikasi ini sangat membantu, fiturnya lengkap.\",  # Expected: Positive\n",
    "    \"Sering crash pas dibuka, nyesel download.\",        # Expected: Negative\n",
    "    \"Biasa aja sih, standar.\"                           # Expected: Neutral\n",
    "]\n",
    "\n",
    "# Label mapping\n",
    "label_map = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üß™ TESTING INFERENCE WITH MANUAL SENTENCES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"  Original: {sentence}\")\n",
    "    \n",
    "    # Preprocess\n",
    "    cleaned_sentence = preprocess_text(sentence)\n",
    "    print(f\"  Cleaned:  {cleaned_sentence}\")\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = loaded_tokenizer(\n",
    "        cleaned_sentence,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        prediction = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    # Get probabilities for each class\n",
    "    probs = probabilities[0].cpu().numpy()\n",
    "    \n",
    "    print(f\"  Predicted Sentiment: {label_map[prediction]}\")\n",
    "    print(f\"  Confidence Scores:\")\n",
    "    print(f\"    - Negative: {probs[0]:.4f} ({probs[0]*100:.2f}%)\")\n",
    "    print(f\"    - Neutral:  {probs[1]:.4f} ({probs[1]*100:.2f}%)\")\n",
    "    print(f\"    - Positive: {probs[2]:.4f} ({probs[2]*100:.2f}%)\")\n",
    "    print(\"\\n\" + \"-\"*70 + \"\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ INFERENCE TESTING COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30412fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üé≤ TESTING WITH RANDOM SAMPLES FROM TEST DATASET\n",
      "======================================================================\n",
      "\n",
      "Sample:\n",
      "  Text: menyesal saya beli vivo , kamera nya tidak bagus .\n",
      "  True Label: Negative\n",
      "  Predicted Label: Negative\n",
      "  ‚úÖ Correct!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Sample:\n",
      "  Text: bicara kemiskinan tetapi hidup dengan kemewahan . bicara kesenjangan tetapi harta triliunan . bicara...\n",
      "  True Label: Negative\n",
      "  Predicted Label: Negative\n",
      "  ‚úÖ Correct!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Sample:\n",
      "  Text: saya sudah bayar tetapi etiket tidak dikirim malah kadaluwarsa , diminta struk pembayaran saya sudah...\n",
      "  True Label: Negative\n",
      "  Predicted Label: Negative\n",
      "  ‚úÖ Correct!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Sample:\n",
      "  Text: saya bersyukur jokowi bakal jadi presiden selama sepuluh tahun .\n",
      "  True Label: Positive\n",
      "  Predicted Label: Positive\n",
      "  ‚úÖ Correct!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Sample:\n",
      "  Text: hotel ini adalah salah satu hotel yang wajib dikunjungi kalau buka puasa bareng , tempat nya menarik...\n",
      "  True Label: Positive\n",
      "  Predicted Label: Positive\n",
      "  ‚úÖ Correct!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with random samples from test dataset\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üé≤ TESTING WITH RANDOM SAMPLES FROM TEST DATASET\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "import random\n",
    "\n",
    "# Get 5 random samples\n",
    "random_indices = random.sample(range(len(dataset['test'])), 5)\n",
    "\n",
    "for idx in random_indices:\n",
    "    sample = dataset['test'][idx]\n",
    "    text = sample['text']\n",
    "    true_label = sample['label']\n",
    "    \n",
    "    print(f\"Sample:\")\n",
    "    print(f\"  Text: {text[:100]}{'...' if len(text) > 100 else ''}\")\n",
    "    print(f\"  True Label: {label_map[true_label]}\")\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = loaded_tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        prediction = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    print(f\"  Predicted Label: {label_map[prediction]}\")\n",
    "    print(f\"  ‚úÖ Correct!\" if prediction == true_label else \"  ‚ùå Incorrect!\")\n",
    "    print(\"\\n\" + \"-\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d253a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4273ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ UPLOADING INDONESIAN SENTIMENT MODEL TO HUGGINGFACE HUB\n",
      "======================================================================\n",
      "\n",
      "üìù Get your token from: https://huggingface.co/settings/tokens\n",
      "‚úÖ Login successful!\n",
      "\n",
      "üì¶ Repository: rkkzone/roberta-sentiment-indonesian-playstore\n",
      "üìÅ Model folder: saved_model_id (~475MB)\n",
      "\n",
      "Files to upload:\n",
      "  - config.json (0.00 MB)\n",
      "  - model.safetensors (474.74 MB)\n",
      "  - special_tokens_map.json (0.00 MB)\n",
      "  - tokenizer.json (0.68 MB)\n",
      "  - tokenizer_config.json (0.00 MB)\n",
      "  - training_args.bin (0.01 MB)\n",
      "  - vocab.txt (0.22 MB)\n",
      "‚úÖ Login successful!\n",
      "\n",
      "üì¶ Repository: rkkzone/roberta-sentiment-indonesian-playstore\n",
      "üìÅ Model folder: saved_model_id (~475MB)\n",
      "\n",
      "Files to upload:\n",
      "  - config.json (0.00 MB)\n",
      "  - model.safetensors (474.74 MB)\n",
      "  - special_tokens_map.json (0.00 MB)\n",
      "  - tokenizer.json (0.68 MB)\n",
      "  - tokenizer_config.json (0.00 MB)\n",
      "  - training_args.bin (0.01 MB)\n",
      "  - vocab.txt (0.22 MB)\n",
      "\n",
      "üì¶ Creating repository: rkkzone/roberta-sentiment-indonesian-playstore\n",
      "\n",
      "üì¶ Creating repository: rkkzone/roberta-sentiment-indonesian-playstore\n",
      "‚¨ÜÔ∏è  Uploading files from saved_model_id/...\n",
      "‚è≥ This may take 5-10 minutes for ~475MB...\n",
      "‚¨ÜÔ∏è  Uploading files from saved_model_id/...\n",
      "‚è≥ This may take 5-10 minutes for ~475MB...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06dc7879a038450c9c8697489b9bb975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db779a8313964fed9225b85d19872ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Upload successful!\n",
      "üîó Model available at: https://huggingface.co/rkkzone/roberta-sentiment-indonesian-playstore\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# üöÄ UPLOAD MODEL TO HUGGINGFACE HUB\n",
    "# ============================================\n",
    "\n",
    "from huggingface_hub import HfApi, login\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "USERNAME = \"rkkzone\"  # Change this to your HuggingFace username\n",
    "REPO_NAME = \"indobert-sentiment-indonesian-playstore\"  # FIXED: IndoBERT not RoBERTa\n",
    "MODEL_FOLDER = \"saved_model_id\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üöÄ UPLOADING INDONESIAN SENTIMENT MODEL TO HUGGINGFACE HUB\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get token\n",
    "print(\"\\nüìù Get your token from: https://huggingface.co/settings/tokens\")\n",
    "TOKEN = input(\"Paste your HuggingFace token: \").strip()\n",
    "\n",
    "# Login\n",
    "try:\n",
    "    login(token=TOKEN)\n",
    "    print(\"‚úÖ Login successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Login failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Prepare upload\n",
    "api = HfApi()\n",
    "repo_id = f\"{USERNAME}/{REPO_NAME}\"\n",
    "\n",
    "print(f\"\\nüì¶ Repository: {repo_id}\")\n",
    "print(f\"üìÅ Model folder: {MODEL_FOLDER} (~475MB)\")\n",
    "print(\"\\nFiles to upload:\")\n",
    "for file in os.listdir(MODEL_FOLDER):\n",
    "    size = os.path.getsize(os.path.join(MODEL_FOLDER, file)) / (1024**2)\n",
    "    print(f\"  - {file} ({size:.2f} MB)\")\n",
    "\n",
    "confirm = input(\"\\n‚ö†Ô∏è  Proceed with upload? (y/n): \").strip().lower()\n",
    "if confirm != 'y':\n",
    "    print(\"‚ùå Upload cancelled\")\n",
    "else:\n",
    "    # Create repo\n",
    "    print(f\"\\nüì¶ Creating repository: {repo_id}\")\n",
    "    api.create_repo(repo_id=repo_id, repo_type=\"model\", exist_ok=True, private=False)\n",
    "    \n",
    "    # Upload folder\n",
    "    print(f\"‚¨ÜÔ∏è  Uploading files from {MODEL_FOLDER}/...\")\n",
    "    print(\"‚è≥ This may take 5-10 minutes for ~475MB...\")\n",
    "    \n",
    "    api.upload_folder(\n",
    "        folder_path=MODEL_FOLDER,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Upload Indonesian sentiment model (IndoBERT) trained on SMSA dataset (11K reviews)\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Upload successful!\")\n",
    "    print(f\"üîó Model available at: https://huggingface.co/{repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d39a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ README created successfully!\n",
      "\n",
      "üéâ All done! Visit your model at:\n",
      "   https://huggingface.co/rkkzone/roberta-sentiment-indonesian-playstore\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# üìù CREATE README FOR HUGGINGFACE MODEL\n",
    "# ============================================\n",
    "\n",
    "readme_content = f\"\"\"---\n",
    "language: id\n",
    "license: mit\n",
    "tags:\n",
    "- sentiment-analysis\n",
    "- indobert\n",
    "- indonesian\n",
    "- google-play-reviews\n",
    "- text-classification\n",
    "datasets:\n",
    "- smsa\n",
    "metrics:\n",
    "- accuracy\n",
    "- f1\n",
    "base_model: indobenchmark/indobert-base-p1\n",
    "model_type: bert\n",
    "---\n",
    "\n",
    "# {REPO_NAME}\n",
    "\n",
    "Fine-tuned **IndoBERT** model for Indonesian sentiment analysis on Google Play Store reviews.\n",
    "\n",
    "## Model Description\n",
    "\n",
    "This model is based on **IndoBERT** (`indobenchmark/indobert-base-p1`) and performs 3-class sentiment classification:\n",
    "- **Positive** (label: 2) üòä\n",
    "- **Neutral** (label: 1) üòê\n",
    "- **Negative** (label: 0) üòû\n",
    "\n",
    "## Training Data\n",
    "\n",
    "- **Dataset**: SMSA (Sentiment Analysis on Indonesian Movie Reviews)\n",
    "- **Language**: Indonesian (Bahasa Indonesia)\n",
    "- **Size**: 11,000 reviews\n",
    "  - Positive: 6,416 reviews (58.3%)\n",
    "  - Negative: 3,436 reviews (31.2%)\n",
    "  - Neutral: 1,148 reviews (10.4%)\n",
    "- **Domain**: App reviews (Google Play Store)\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"{repo_id}\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Example Indonesian review\n",
    "text = \"Aplikasi bagus sekali! Sangat direkomendasikan.\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    sentiment = torch.argmax(predictions, dim=-1).item()\n",
    "\n",
    "sentiment_labels = {{0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}}\n",
    "print(f\"Sentiment: {{sentiment_labels[sentiment]}}\")\n",
    "print(f\"Confidence: {{predictions[0][sentiment].item():.4f}}\")\n",
    "```\n",
    "\n",
    "### Batch Processing\n",
    "\n",
    "```python\n",
    "reviews = [\n",
    "    \"Aplikasi bagus, mudah digunakan!\",\n",
    "    \"Tidak bisa login, aplikasi error terus\",\n",
    "    \"Biasa aja sih\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(reviews, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    sentiments = torch.argmax(predictions, dim=-1)\n",
    "\n",
    "for review, sentiment, probs in zip(reviews, sentiments, predictions):\n",
    "    label = sentiment_labels[sentiment.item()]\n",
    "    conf = probs[sentiment].item()\n",
    "    print(f\"Review: {{review}}\")\n",
    "    print(f\"Sentiment: {{label}} ({{conf:.2%}} confident)\\\\n\")\n",
    "```\n",
    "\n",
    "## Model Performance\n",
    "\n",
    "**Training Configuration:**\n",
    "- **Base Model**: IndoBERT (`indobenchmark/indobert-base-p1`)\n",
    "- **Architecture**: BERT-based encoder\n",
    "- **Training Epochs**: 3\n",
    "- **Batch Size**: 16\n",
    "- **Learning Rate**: 2e-5\n",
    "- **Max Length**: 128 tokens\n",
    "- **Optimizer**: AdamW\n",
    "- **Weight Decay**: 0.01\n",
    "\n",
    "**Evaluation Results:**\n",
    "- Check model card for detailed metrics on test set\n",
    "- Includes per-class precision, recall, and F1 scores\n",
    "\n",
    "## Dataset Preprocessing\n",
    "\n",
    "The model was trained with the following preprocessing:\n",
    "1. **Indonesian Slang Normalization**: Common abbreviations expanded (gak‚Üítidak, bgd‚Üíbanget, etc.)\n",
    "2. **Text Cleaning**: Remove special characters, URLs, excessive whitespace\n",
    "3. **Tokenization**: IndoBERT tokenizer with max 128 tokens\n",
    "4. **Label Mapping**: \n",
    "   - `negative` ‚Üí 0\n",
    "   - `neutral` ‚Üí 1\n",
    "   - `positive` ‚Üí 2\n",
    "\n",
    "## Intended Use\n",
    "\n",
    "This model is optimized for:\n",
    "- ‚úÖ Indonesian language app reviews\n",
    "- ‚úÖ Google Play Store sentiment analysis\n",
    "- ‚úÖ Customer feedback classification\n",
    "- ‚úÖ Review monitoring and analytics\n",
    "- ‚úÖ E-commerce product reviews (Indonesian)\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Optimized for **informal Indonesian** text (app reviews)\n",
    "- May not generalize well to formal Indonesian literature\n",
    "- Best performance on **short texts** (< 128 tokens)\n",
    "- Sensitive to **slang variations** across Indonesian regions\n",
    "- Trained primarily on app domain data\n",
    "\n",
    "## Applications\n",
    "\n",
    "### 1. **App Analytics Dashboard**\n",
    "Analyze user sentiment in Indonesian app reviews to track satisfaction over time.\n",
    "\n",
    "### 2. **Customer Support Prioritization**\n",
    "Automatically flag negative reviews for urgent response.\n",
    "\n",
    "### 3. **Market Research**\n",
    "Understand Indonesian user preferences and pain points.\n",
    "\n",
    "### 4. **Review Filtering**\n",
    "Filter out spam or irrelevant reviews based on sentiment patterns.\n",
    "\n",
    "## Example Predictions\n",
    "\n",
    "| Review (Indonesian) | Predicted Sentiment | Confidence |\n",
    "|---------------------|---------------------|------------|\n",
    "| \"Aplikasi keren banget! Fiturnya lengkap!\" | Positive | 95.3% |\n",
    "| \"Aplikasi sering crash, mohon diperbaiki\" | Negative | 87.2% |\n",
    "| \"Lumayan lah, standar aja\" | Neutral | 72.8% |\n",
    "| \"Mantap jiwa! Recommended!\" | Positive | 91.5% |\n",
    "| \"Mengecewakan, buang-buang waktu\" | Negative | 89.6% |\n",
    "\n",
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@misc{{indobert_sentiment_indonesian_playstore,\n",
    "  author = {{{USERNAME}}},\n",
    "  title = {{Indonesian Sentiment Analysis for Google Play Reviews (IndoBERT)}},\n",
    "  year = {{2025}},\n",
    "  publisher = {{Hugging Face}},\n",
    "  url = {{https://huggingface.co/{repo_id}}},\n",
    "  note = {{Fine-tuned IndoBERT for Indonesian app review sentiment analysis}}\n",
    "}}\n",
    "```\n",
    "\n",
    "## License\n",
    "\n",
    "MIT License - Free for commercial and non-commercial use.\n",
    "\n",
    "## Related Resources\n",
    "\n",
    "- **Base Model**: [IndoBERT](https://huggingface.co/indobenchmark/indobert-base-p1)\n",
    "- **Dataset**: [SMSA Indonesian Sentiment](https://github.com/IndoNLP/indonlu)\n",
    "- **Live Demo**: [Google Play Review Analyzer](https://google-play-review-analyzer.streamlit.app)\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "- Dataset: Indonesian NLP research community (SMSA dataset)\n",
    "- Base Model: IndoNLP team for IndoBERT (`indobenchmark/indobert-base-p1`)\n",
    "- Framework: Hugging Face Transformers\n",
    "\n",
    "---\n",
    "\n",
    "**Built with ‚ù§Ô∏è for Indonesian app developers**\n",
    "\n",
    "*For issues or questions, please open an issue on the model repository.*\n",
    "\"\"\"\n",
    "\n",
    "# Upload README\n",
    "try:\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=readme_content.encode(),\n",
    "        path_in_repo=\"README.md\",\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Add comprehensive README with usage examples and model details (IndoBERT-based)\"\n",
    "    )\n",
    "    print(\"\\n‚úÖ README created successfully!\")\n",
    "    print(f\"\\nüéâ All done! Visit your model at:\")\n",
    "    print(f\"   https://huggingface.co/{repo_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå README upload failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797feb05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
